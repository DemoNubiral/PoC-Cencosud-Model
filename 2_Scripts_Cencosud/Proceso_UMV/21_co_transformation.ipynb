{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90bd87f1-af62-4a06-b850-e429f6b4d3a5",
   "metadata": {},
   "source": [
    "# **<font color='black'>COLOMBIA</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c5ab0b-3cae-424e-8649-e7716151d08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dotenv installed\n",
      "Loaded variables .env True\n",
      "Pyspark installed\n",
      "JAVA:  /usr/lib/jvm/temurin-8-jdk-amd64\n",
      "DATAPROC:  True\n",
      "Current GCP Project Name: cencosudx\n",
      "Current ENVIRONMENT: staging\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pyspark\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import date, datetime, timedelta, date\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import scipy.stats as sps\n",
    "import seaborn as sns\n",
    "import matplotlib as m\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib.pylab import rcParams\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import requests\n",
    "from statsforecast.arima import AutoARIMA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import sklearn\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Paquetes (checar o que preciso e o que nÃ£o preciso)\n",
    "import pyspark.sql.functions as F\n",
    "# from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "#logging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR)\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from gcpspark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_date, date_sub, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0fdf754-9809-41ce-a39d-c963fccb1bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = os.environ[\"ENVIRONMENT\"]\n",
    "BUCKET=f\"{ENV}-bucket-dataproc-bigquery/dataproc/co_transformation\"\n",
    "# TABLE=\"data_analytics_default.BQ_CO_SM_SALES_02\"\n",
    "dataset_id = 'data_analytics_default'\n",
    "table_id = 'BQ_CO_SM_SALES_01'\n",
    "table_id2 = 'BQ_CO_SM_SALES_02'\n",
    "TABLE=f\"{dataset_id}.{table_id}\"\n",
    "TABLE2=f\"{dataset_id}.{table_id2}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3594f9a9-dbe0-4c06-865a-e0002ba88409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at: /jars/spark-bigquery-with-dependencies_2.12-0.26.0.jar\n",
      "File already exists at: /jars/gcs-connector-hadoop3-2.2.19.jar\n",
      "Process: co_umv_transformation_1716241077788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: viewsEnabled\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Spark Session\n",
    "spark = create_pyspark(name=\"co_umv_transformation\", connection=\"GCP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc750f07-9a05-4089-98a6-01fe40655723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- canal_venta: string (nullable = true)\n",
      " |-- tienda_venta: string (nullable = true)\n",
      " |-- sku_venta: string (nullable = true)\n",
      " |-- ean_venta: string (nullable = true)\n",
      " |-- fecha_venta: date (nullable = true)\n",
      " |-- hora_venta: string (nullable = true)\n",
      " |-- transaccion_venta: string (nullable = true)\n",
      " |-- pos_venta: string (nullable = true)\n",
      " |-- unidades_venta: double (nullable = true)\n",
      " |-- unidades_umb_venta: string (nullable = true)\n",
      " |-- monto_neto_venta: double (nullable = true)\n",
      " |-- monto_costo_venta: double (nullable = true)\n",
      " |-- id_promocion: string (nullable = true)\n",
      " |-- monto_promocion: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- EAN: string (nullable = true)\n",
      " |-- ITEM_ID: string (nullable = true)\n",
      " |-- LOCAL_ID: string (nullable = true)\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- UNITS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data collection\n",
    "df = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"table\", TABLE)\n",
    "    .load()\n",
    ")\n",
    "# print(df.count())\n",
    "df = df.drop(\"_PARTITIONTIME\", \"_PARTITIONDATE\")\n",
    "df.printSchema()\n",
    "\n",
    "# Data collection\n",
    "df_historical = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"table\", TABLE2)\n",
    "    .load()\n",
    ")\n",
    "# print(df.count())\n",
    "df_historical = df_historical.drop(\"_PARTITIONTIME\", \"_PARTITIONDATE\")\n",
    "df_historical.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f2e767b-8d22-4f0a-bcd4-445f4ee8c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates = df.groupBy(df.columns).count().filter(\"count > 1\")\n",
    "# num_duplicates = duplicates.count()\n",
    "\n",
    "# if num_duplicates > 0:\n",
    "#     print(f\"The dataset contains {num_duplicates} duplicate rows.\")\n",
    "#     duplicates.show()\n",
    "# else:\n",
    "#     print(\"The dataset does not contain any duplicate rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e3b67b3-0360-4dfb-aa08-db924ba42b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"ean_venta\", \"sku_venta\", \"tienda_venta\", \"fecha_venta\", \"unidades_venta\")\n",
    "df = df.withColumnRenamed(\"tienda_venta\",\"LOCAL_ID\")\\\n",
    "        .withColumnRenamed(\"sku_venta\",\"ITEM_ID\")\\\n",
    "        .withColumnRenamed(\"ean_venta\",\"EAN\")\\\n",
    "        .withColumnRenamed(\"fecha_venta\",\"DATE\")\\\n",
    "        .withColumnRenamed(\"unidades_venta\",\"UNITS\")\\\n",
    "        .filter(F.col(\"UNITS\")>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07bf730-90a6-438b-8341-86e8e699551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EAN: string (nullable = true)\n",
      " |-- ITEM_ID: string (nullable = true)\n",
      " |-- LOCAL_ID: string (nullable = true)\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- UNITS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78fc992f-c4aa-484f-88c6-3193d5b7d9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"bigquery\") \\\n",
    "    .option(\"temporaryGcsBucket\",BUCKET) \\\n",
    "    .option(\"table\", TABLE2) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "093b5fde-c82f-44a0-a839-57a7c319e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_sales = df.select(F.sum(\"UNITS\")).collect()[0][0]\n",
    "# average_sales = df.select(F.avg(\"UNITS\")).collect()[0][0]\n",
    "# min_sales = df.select(F.min(\"UNITS\")).collect()[0][0]\n",
    "# max_sales = df.select(F.max(\"UNITS\")).collect()[0][0]\n",
    "# std_dev_sales = df.select(F.stddev(\"UNITS\")).collect()[0][0]\n",
    "# total_records = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6054248b-9a9c-4669-b1c2-c86f7007e8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a283ac-97e6-43bc-a592-fb99c75f2819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m107"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
