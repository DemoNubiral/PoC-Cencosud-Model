{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "import dotenv\n",
    "from gcpspark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_date, date_sub, col\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = os.environ[\"ENVIRONMENT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "properties={\"accesskey\":os.getenv('colombia_sm_aws_access_key_id'),\"secretkey\":os.getenv('colombia_sm_aws_secret_access_key'),\"sas_url\":\"\",\"sas\":\"\"}\n",
    "spark_name = \"data-extraction-br\"\n",
    "spark = create_pyspark(name = spark_name , connection=\"s3\", properties=properties, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_stock(df):\n",
    "    return df.select(\"tienda_stock\",\"sku_stock\",\"ean_stock\",\"fecha_stock\",\"unidades_stock\").filter(F.col(\"fecha_stock\") == F.date_sub(F.current_date(), 1)).filter(F.col(\"unidades_stock\")>0)\n",
    "    \n",
    "def transformation_active(df):\n",
    "     return df.filter(F.col(\"estado_producto\") == \"S\").drop(\"estado_producto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data overwritten!\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'cencosud.prod.sandbox.sm.col'\n",
    "# s3://cencosud.prod.sandbox.sm.col/ANALYTICS/ARCUS/\n",
    "# s3://cencosud.prod.super.peru.analytics/ExternalAccess/arcus_smk_pe/automatico/vw_milocal_fact_stock_mat\n",
    "sources = [    \n",
    "    {\n",
    "        \"prefix\":\"ANALYTICS/ARCUS/Maestra_Activa_Colombia*\",\n",
    "        \"bucket\":f\"{ENV}-bucket-dataproc-bigquery/dataproc/co_extraction\",\n",
    "        \"table\":\"data_analytics_default.BQ_CO_SM_ACTIVE_01\",\n",
    "        \"mode\":\"overwrite\",\n",
    "        \"transformation\":transformation_active\n",
    "    },\n",
    "    {\n",
    "        \"prefix\":\"ANALYTICS/ARCUS/Maestra_Canal_Ventas_Colombia*\",\n",
    "        \"bucket\":f\"{ENV}-bucket-dataproc-bigquery/dataproc/co_extraction\",\n",
    "        \"table\":\"data_analytics_default.BQ_CO_SM_CHANNELS_01\",\n",
    "        \"mode\":\"overwrite\"\n",
    "    },\n",
    "    {\n",
    "        \"prefix\":\"ANALYTICS/ARCUS/Maestra_Promocion_Colombia*\",\n",
    "        \"bucket\":f\"{ENV}-bucket-dataproc-bigquery/dataproc/co_extraction\",\n",
    "        \"table\":\"data_analytics_default.BQ_CO_SM_PROMOTIONS_01\",\n",
    "        \"mode\":\"overwrite\"\n",
    "    },\n",
    "    {\n",
    "        \"prefix\":\"ANALYTICS/ARCUS/Maestra_Tienda_Colombia*\",\n",
    "        \"bucket\":f\"{ENV}-bucket-dataproc-bigquery/dataproc/co_extraction\",\n",
    "        \"table\":\"data_analytics_default.BQ_CO_SM_STORES_01\",\n",
    "        \"mode\":\"overwrite\"\n",
    "    },\n",
    "    {\n",
    "        \"prefix\":\"ANALYTICS/ARCUS/Productos_Arcus_Colombia*\",\n",
    "        \"bucket\":f\"{ENV}-bucket-dataproc-bigquery/dataproc/co_extraction\",\n",
    "        \"table\":\"data_analytics_default.BQ_CO_SM_PRODUCTS_01\",\n",
    "        \"mode\":\"append\"\n",
    "    },\n",
    "    {\n",
    "        \"prefix\":\"ANALYTICS/ARCUS/Ventas_Arcus_Colombia*\",\n",
    "        \"bucket\":f\"{ENV}-bucket-dataproc-bigquery/dataproc/co_extraction\",\n",
    "        \"table\":\"data_analytics_default.BQ_CO_SM_SALES_01\",\n",
    "        \"mode\":\"append\"\n",
    "    },\n",
    "    {\n",
    "        \"prefix\":\"ANALYTICS/ARCUS/Stock_Arcus_Colombia*\",\n",
    "        \"bucket\":f\"{ENV}-bucket-dataproc-bigquery/dataproc/co_extraction\",\n",
    "        \"table\":\"data_analytics_default.BQ_CO_SM_STOCK_01\",\n",
    "        \"mode\":\"overwrite\",\n",
    "        \"transformation\":transformation_stock\n",
    "    }\n",
    "]\n",
    "for source in sources:\n",
    "    output_table_name = source[\"table\"]\n",
    "    prefix = source[\"prefix\"]\n",
    "    print(f\"Initialization: {output_table_name}\")\n",
    "    len_new_data = 0\n",
    "    counter = 17\n",
    "    while (len_new_data==0 and counter>0):\n",
    "        new_data = spark.read\\\n",
    "            .parquet(f\"s3a://{bucket_name}/{prefix}\")\n",
    "\n",
    "        if(\"transformation\" in source.keys()):\n",
    "            new_data = source[\"transformation\"](new_data)\n",
    "\n",
    "        len_new_data = new_data.count()\n",
    "        print(\"# new data: \", len_new_data)\n",
    "\n",
    "        if(len_new_data==0):\n",
    "            print(\"----> WARN: Not uploaded to bigquery because no data.\")\n",
    "            time.sleep(60*10)\n",
    "            counter=counter-1\n",
    "\n",
    "    if(source[\"mode\"]==\"overwrite\"):\n",
    "        new_data.write.format(\"bigquery\") \\\n",
    "            .option(\"temporaryGcsBucket\",source[\"bucket\"]) \\\n",
    "            .option(\"table\", output_table_name) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "        print(\"Data overwritten!\")\n",
    "        continue\n",
    "    # Read the historical data from the BigQuery table into a DataFrame\n",
    "    historical_data = spark.read.format(\"bigquery\") \\\n",
    "        .option(\"table\", output_table_name) \\\n",
    "        .load()\n",
    "    \n",
    "    historical_data = historical_data.drop(\"_PARTITIONTIME\", \"_PARTITIONDATE\")\n",
    "\n",
    "    # Identify the new data that doesn't exist in the filtered historical data\n",
    "    new_data_unique = new_data.exceptAll(historical_data)\n",
    "\n",
    "    # Append the new unique data to the historical data in BigQuery\n",
    "    # combined_data = filtered_historical_data.union(new_data_unique)\n",
    "    n_new_data = new_data_unique.count()\n",
    "    print(\"# historical data: \", historical_data.count())\n",
    "    print(\"# new unique data: \", n_new_data)\n",
    "    \n",
    "    if(n_new_data==0):\n",
    "        print(\"----> WARN: Not uploaded to bigquery because no unique data.\")\n",
    "        continue\n",
    "    new_data_unique.write.format(\"bigquery\") \\\n",
    "        .option(\"temporaryGcsBucket\",source[\"bucket\"]) \\\n",
    "        .option(\"table\", output_table_name) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
