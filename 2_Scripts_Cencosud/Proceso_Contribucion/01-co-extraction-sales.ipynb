{"cells": [{"cell_type": "markdown", "id": "67e686b0-5b7d-4288-bfaf-cc988f9f55ab", "metadata": {}, "source": "# Libraries"}, {"cell_type": "code", "execution_count": 2, "id": "a47b3ee5-1b04-4509-a2d4-b1472e5e9917", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Dotenv installed\nLoaded variables .env True\nPyspark installed\nJAVA:  /usr/lib/jvm/temurin-8-jdk-amd64\nDATAPROC:  True\nCurrent GCP Project Name: cencosudx\nCurrent ENVIRONMENT: staging\n<class 'Exception'>\n"}], "source": "import os\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import *\nimport pandas as pd\nfrom gcpspark import *"}, {"cell_type": "code", "execution_count": 3, "id": "23bb01bf-c7a2-48e8-b049-cde626bbeac9", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "File already exists at: /jars/spark-bigquery-with-dependencies_2.12-0.26.0.jar\nFile already exists at: /jars/gcs-connector-hadoop3-2.2.19.jar\nProcess: spark_co_extraction_sales_1746115922854\n"}, {"name": "stderr", "output_type": "stream", "text": "Warning: Ignoring non-Spark config property: viewsEnabled\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/05/01 16:12:07 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n25/05/01 16:12:07 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n25/05/01 16:12:07 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n25/05/01 16:12:07 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "spark_name = \"spark_co_extraction_sales\"\nspark = create_pyspark(name = spark_name , connection=\"GCP\", verbose=True)"}, {"cell_type": "code", "execution_count": 4, "id": "89bd0f14-5b69-483a-b68f-4a65e8be85b5", "metadata": {}, "outputs": [], "source": "table_id = 'cencosudx.data_analytics_default.BQ_CO_SM_SALES_01'"}, {"cell_type": "code", "execution_count": 5, "id": "82aa7234-19c1-4069-bfc1-e592ac295d12", "metadata": {}, "outputs": [], "source": "df = spark.read \\\n    .format(\"bigquery\") \\\n    .option(\"table\",table_id)\\\n    .load()"}, {"cell_type": "code", "execution_count": 6, "id": "684bf618-2521-4d2d-ac6c-a9e76399c4cb", "metadata": {}, "outputs": [], "source": "df = df.select(\n    F.col(\"ean_venta\").alias(\"EAN\"),\n    F.col(\"sku_venta\").alias(\"Item_Id\"),\n    F.col(\"tienda_venta\").alias(\"Location_Id\"),\n    F.col(\"fecha_venta\").alias(\"Date\"),\n    F.col(\"hora_venta\").alias(\"Hour_Sales\"),\n    F.col(\"canal_venta\").alias(\"Sales_Channel\"),\n    F.col(\"unidades_venta\").alias(\"Quantity_Sales\"),\n    F.col(\"monto_neto_venta\").alias(\"NetAmount\"),\n    F.col(\"monto_costo_venta\").alias(\"NetCost\")\n).distinct()"}, {"cell_type": "code", "execution_count": 7, "id": "4d605449-e839-4754-86f6-659f403fa70e", "metadata": {}, "outputs": [], "source": "import pyspark.sql.functions as F\n#new_data_1 = df.filter(F.col('fecha_venta') >= '2025-01-01')"}, {"cell_type": "code", "execution_count": null, "id": "f4c5986e-1024-4c86-9b2a-3ac3a16c586e", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df.write.mode(\"overwrite\").parquet(\"gs://staging-da-contribution/CO/sales\")\n\n#df.write.format(\"bigquery\").option(\"table\",\"data_analytics_default.BQ_CO_SM_RAW_01\")\\\n#    .option(\"temporaryGcsBucket\",\"staging-bucket-dataproc-bigquery/dataproc/co_umv\").mode(\"overwrite\").save()\n\n"}, {"cell_type": "code", "execution_count": null, "id": "43e849ab-511e-4a76-b391-75af2256c046", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 9, "id": "8598de21-477e-4c15-8cea-233c5d5c50f0", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "DONE\n"}], "source": "print(\"DONE\")"}, {"cell_type": "code", "execution_count": null, "id": "21577ce0-0ce8-45b6-9ba1-6752bcd31bab", "metadata": {}, "outputs": [], "source": "df = df.withColumn(\"Date\", F.to_date(\"Date\"))\ndf_sku_por_mes = df.groupBy(F.year(\"Date\").alias(\"year\"), F.month(\"Date\").alias(\"month\")).agg(F.countDistinct(\"Item_Id\").alias(\"cant_sku\"))\ndf_sku_por_mes = df_sku_por_mes.orderBy(\"year\", \"month\")\ndf_sku_por_mes.show(50) "}, {"cell_type": "code", "execution_count": 10, "id": "a29f7e89-c45b-4e7c-b5da-58539e1be3d7", "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": 11, "id": "dbcb9eef-9b4d-4fa6-bd9b-52e150843f75", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "ok\n"}], "source": "print(\"ok\")"}, {"cell_type": "code", "execution_count": null, "id": "54dac4f3-5a80-46b4-a2d7-6f87e2f72619", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"environment": {"kernel": "python3", "name": "tf2-gpu.2-11.m104", "type": "gcloud", "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m104"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}